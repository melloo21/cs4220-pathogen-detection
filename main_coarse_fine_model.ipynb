{"cells":[{"cell_type":"markdown","metadata":{"id":"rLODzMyA57I6"},"source":["# CS4220 Project 2 - Pathogen Detection\n","\n","In this notebook, we give an example to show how to read and use the DNA read data. We will train one base line model, using `sklearn.LogisticRegression`, and use it to predict the pathogens in each patient's dataset.\n","\n","## Related python packages\n","\n","To get started (if you are using python), you may want to create a virtual python environment and install some packages. Here are some of the commands you might need:\n","\n","```bash\n","conda create --name cs4220 python=3.8\n","\n","# Install jupyter notebook if you are using it\n","conda install -c anaconda ipykernel\n","python -m ipykernel install --user --name=cs4220\n","conda install -c anaconda jupyter\n","\n","# Some common packages\n","conda install pandas                      # for reading csv\n","conda install scikit-learn                # for the logistic regression model\n","pip install pytorch                       # if you are using neural networks\n","conda install -c conda-forge matplotlib   # for plotting\n","conda install seaborn                     # also for plotting\n","pip install umap-learn[plot]              # plotting UMAP plots\n","conda install numpy                       # for many math/vectorized operations\n","```"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"6T_tuslc57JA"},"outputs":[],"source":["# import packages\n","\n","import numpy as np\n","import pandas as pd\n","import timeit\n","import time\n","import statistics \n","import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pickle as pkl\n","from umap.umap_ import UMAP\n","from joblib import dump, load\n","from itertools import chain\n","from typing import Union, Any\n","\n","from sklearn import preprocessing\n","from sklearn.decomposition import PCA\n","from sklearn.linear_model import LogisticRegression\n","\n","# Non-linear classifiers\n","from sklearn import svm # Should do with RBF or polynomial kernel\n","from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier \n","# Note OVR -- sensitive to imbalanced dataset, OVO is less sensitive\n","from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n","from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n","from sklearn.neighbors import (NeighborhoodComponentsAnalysis, KNeighborsClassifier)\n","\n","from utils.common import *\n","from utils.dataset import CS4220Dataset\n","from utils.feature_selection import *\n","from utils.model_evaluation import _read_model, get_all_jaccard_index, performance_evaluate, get_all_jaccard_index_with_transformation"]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/melloo21/opt/anaconda3/envs/cs4220_pathogen_detection/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","/Users/melloo21/opt/anaconda3/envs/cs4220_pathogen_detection/lib/python3.8/site-packages/umap/plot.py:203: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n","  @numba.jit(nopython=False)\n"]}],"source":["import datashader as ds\n","import datashader.transfer_functions as tf\n","import datashader.bundling as bd\n","import colorcet\n","import matplotlib.cm\n","import bokeh.plotting as bpl\n","import bokeh.transform as btr\n","import holoviews as hv\n","import holoviews.operation.datashader as hd\n","\n","import umap.plot\n","import torch\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader"]},{"cell_type":"code","execution_count":11,"metadata":{"id":"IUeiOrYl57JD","outputId":"3587672f-8d2a-48d7-a640-ab33ac23e62b"},"outputs":[{"data":{"text/plain":["'/Users/melloo21/Desktop/NUS Items/CS4220/cs4220_project_2/cs4220-pathogen-detection'"]},"execution_count":11,"metadata":{},"output_type":"execute_result"}],"source":["# Check the current working directory\n","%pwd"]},{"cell_type":"markdown","metadata":{},"source":["## Testing Utils"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["\n","def random_forest_selector(X,y):\n","    model = RandomForestClassifier(random_state=1, max_depth=10)\n","    model.fit(X,y)\n","    features = df.columns\n","    importances = model.feature_importances_\n","    indices = np.argsort(importances)[-9:]  # top 10 features\n","    plt.title('Feature Importances')\n","    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n","    plt.yticks(range(len(indices)), [features[i] for i in indices])\n","    plt.xlabel('Relative Importance')\n","    plt.show()\n","\n","def create_coarse_labels(df):\n","    df['coarse_species_name'] = np.where(df['species_name'] != \"decoy\", \"non_decoy\", \"decoy\")\n","    le = preprocessing.LabelEncoder()\n","    le.fit(df['coarse_species_name'].unique())\n","    y_index = le.transform(df['coarse_species_name'].values)\n","    df['labels'] = y_index\n","    print(f\"Unique labels {len(df['coarse_species_name'].unique())}\")\n","    return df, le\n","\n","def filtered_array(array, idx_to_drop):\n","    return np.take(array, idx_to_drop, axis=1)\n","    \n","def jaccard_index_per_patient(patient_id:str, preds):\n","    # Generic filename\n","    df_true = pd.read_csv('test_data/patient{}_labels.csv'.format(patient_id))\n","    tp, fp, tp_fn = 0, 0, df_true['labels'].shape[0]\n","    print('my predition(s) for patient {}:'.format(patient_id))\n","    print(preds)\n","    print('true pathogen')\n","    print(df_true['labels'].values)\n","    # if don't predict any pathogen, it means there is only decoy in the test dataset (your prediction)\n","    if len(preds) == 0:\n","        preds = ['decoy']\n","    for item in np.unique(preds):\n","        if item in df_true['labels'].values:\n","            tp += 1\n","        else:\n","            fp += 1\n","    #you have to predict all labels correctly, but you are penalized for any false positive\n","    return tp / (tp_fn + fp) , preds, df_true['labels'].values\n","\n","def get_all_jaccard_index_with_filter( model:Any, label_encoder:Any , filtered_idx:Any, num_patients:int=10, threshold:float=0.95):\n","\n","    all_jaccard_index = []\n","    all_pred = []\n","    all_true = []\n","    for patient_id in range(num_patients):\n","        print('predicting for patient {}'.format(patient_id))\n","\n","        with open('test_data/patient{}_6mers.npy'.format(patient_id), 'rb') as read_file:\n","            df_test = np.load(read_file)\n","\n","        # regr.predict relies on argmax, thus predict to every single read and you will end up with many false positives\n","        transformed_data = filtered_array(df_test,filtered_idx)\n","        print(f\"Shape of {transformed_data.shape}\")\n","        y_pred = model.predict(transformed_data)\n","\n","        # we can use regr.predict_proba to find a good threshold and predict only for case where the model is confident.\n","        # here I apply 0.95 as the cutoff for my predictions, let's see how well my model will behave...\n","        y_predprob = model.predict_proba(transformed_data)\n","\n","        # we get only predictions larger than the threshold and if there is more than one, we take the argmax again\n","        final_predictions = label_encoder.inverse_transform(\n","                                np.unique([np.argmax(item) for item in y_predprob if len(np.where(item >= threshold)[0]) >= 1]\n","                            ))\n","\n","        # my pathogens dectected, decoy will be ignored\n","        final_predictions = [item for item in final_predictions if item !='decoy']\n","\n","        ji, pred_pathogen, true_pathogen = jaccard_index_per_patient(patient_id, final_predictions)\n","        print('Jaccard index: {}'.format(ji))\n","        all_jaccard_index.append(ji)    \n","        all_pred.append(pred_pathogen)\n","        all_true.append(true_pathogen)\n","\n","    return all_jaccard_index, flatten(all_pred), flatten(all_true)"]},{"cell_type":"markdown","metadata":{"id":"FJtaUfX_57JE"},"source":["## Parsing Input Data\n","\n","First, some dataset statistics. We load our training labels (ground truth) and see how many reads a category (species) have."]},{"cell_type":"code","execution_count":13,"metadata":{"id":"H2nDX_MC57JF","outputId":"5184abfb-4fad-45ef-b37d-a121e374f81a"},"outputs":[{"data":{"text/plain":["(461581,\n"," species_name\n"," decoy                              413476\n"," burkholderia_pseudomallei            3533\n"," pseudomonas_aeruginosa               3126\n"," klebsiella_michiganensis             2989\n"," mycobacterium_ulcerans               2910\n"," klebsiella_pneumoniae                2806\n"," serratia_liquefaciens                2629\n"," vibrio_parahaemolyticus              2579\n"," salmonella_enterica_typhimurium      2507\n"," yersinia_enterocolitica              2276\n"," stenotrophomonas_maltophilia         2217\n"," mycobacterium_tuberculosis           2175\n"," clostridioides_difficile             2007\n"," acinetobacter_baumannii              1964\n"," legionella_pneumophila               1753\n"," listeria_monocytogenes               1479\n"," staphylococcus_aureus                1384\n"," staphylococcus_pseudintermedius      1328\n"," corynebacterium_ulcerans             1266\n"," corynebacterium_diphtheriae          1194\n"," streptococcus_suis                   1092\n"," neisseria_gonorrhoeae                1087\n"," streptococcus_agalactiae             1060\n"," streptococcus_pneumoniae             1056\n"," staphylococcus_pyogenes               856\n"," campylobacter_jejuni                  832\n"," Name: count, dtype: int64,\n"," species_name\n"," decoy                              0.895782\n"," burkholderia_pseudomallei          0.007654\n"," pseudomonas_aeruginosa             0.006772\n"," klebsiella_michiganensis           0.006476\n"," mycobacterium_ulcerans             0.006304\n"," klebsiella_pneumoniae              0.006079\n"," serratia_liquefaciens              0.005696\n"," vibrio_parahaemolyticus            0.005587\n"," salmonella_enterica_typhimurium    0.005431\n"," yersinia_enterocolitica            0.004931\n"," stenotrophomonas_maltophilia       0.004803\n"," mycobacterium_tuberculosis         0.004712\n"," clostridioides_difficile           0.004348\n"," acinetobacter_baumannii            0.004255\n"," legionella_pneumophila             0.003798\n"," listeria_monocytogenes             0.003204\n"," staphylococcus_aureus              0.002998\n"," staphylococcus_pseudintermedius    0.002877\n"," corynebacterium_ulcerans           0.002743\n"," corynebacterium_diphtheriae        0.002587\n"," streptococcus_suis                 0.002366\n"," neisseria_gonorrhoeae              0.002355\n"," streptococcus_agalactiae           0.002296\n"," streptococcus_pneumoniae           0.002288\n"," staphylococcus_pyogenes            0.001854\n"," campylobacter_jejuni               0.001803\n"," Name: count, dtype: float64)"]},"execution_count":13,"metadata":{},"output_type":"execute_result"}],"source":["label_df = pd.read_csv('./training_data/train_labels.csv')\n","get_species_count(label_df)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique labels 2\n"]}],"source":["coarse_label_df ,coarse_label_encoder = create_coarse_labels(label_df)"]},{"cell_type":"markdown","metadata":{"id":"CLk96Igt57JG"},"source":["It seems that we have a lot of decoy reads (decoy means sequencing reads from human or commensal species)."]},{"cell_type":"code","execution_count":15,"metadata":{"id":"-0pyqyW-57JH"},"outputs":[{"name":"stdout","output_type":"stream","text":["Unique labels 26\n"]}],"source":["# snippet to load the grouth truth training labels and normalize the label predictions.\n","# your trained model will predict in this space (26 classes - pathogens and decoy)\n","processed_label_df ,label_encoder = create_label_df(label_df)\n","#this label_df will have label index instead of the actual class value"]},{"cell_type":"markdown","metadata":{"id":"0WYNIfBH57JI"},"source":["We are not training using the full dataset to provide a quick baseline here. Thus, we subsampled so that each category has 500 reads. Up to you to come up with interesting training strategies to use your full training dataset (or even your own datasets)!"]},{"cell_type":"code","execution_count":16,"metadata":{"id":"rf5L_X3W57JJ","outputId":"cd83b94e-77b8-4cfe-b9d1-8de822a16c3e"},"outputs":[{"data":{"text/plain":["Index([  622,   105,  1635,  1382,   518,   894,  1320,   729,  1759,  1030,\n","       ...\n","       45964, 47308, 47609, 47539, 46336, 46848, 46431, 45957, 47345, 47186],\n","      dtype='int64', length=20800)"]},"metadata":{},"output_type":"display_data"}],"source":["#500 here means that there are at most 500 per label class\n","# find ways to reduce the sample size as there are >10000 samples\n","\n","samples_800_index = create_sampling_idx(label_df,800)\n","display(samples_800_index)"]},{"cell_type":"markdown","metadata":{"id":"4Zg_NzrW57JK"},"source":["In this project, we try to use canonical $k$-mer profiles to represent each read in the input. Here, we are using $k=6$ and consequently 2081 features (including 1 feature `IGNORE` for ambiguous $k$-mer) for each read. Read [this reference](https://bioinfologics.github.io/post/2018/09/17/k-mer-counting-part-i-introduction/) for more information.\n","\n","To help you save time, we implemented a utility class `CS4220Dataset` that can\n","- take in raw reads as input (`.fasta`, `.fa` files), and turn them into $k$-mer profiles, or\n","- take in $k$-mer profile as input (`.npy` files),\n","- allow you to sample data or create $k$-mer profile on the fly (during training) to save memory."]},{"cell_type":"code","execution_count":17,"metadata":{},"outputs":[{"ename":"NameError","evalue":"name 'samples_500_index' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[17], line 5\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example usage\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;66;03m#input_file_path = './training_data/train_raw_reads.fasta'\u001b[39;00m\n\u001b[1;32m      3\u001b[0m input_file_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m./training_data/train_6mers.npy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 5\u001b[0m coarse_sampled_dataset \u001b[38;5;241m=\u001b[39m CS4220Dataset(data_file\u001b[38;5;241m=\u001b[39minput_file_path, label_df\u001b[38;5;241m=\u001b[39mcoarse_label_df, samples_index\u001b[38;5;241m=\u001b[39m\u001b[43msamples_500_index\u001b[49m)\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(coarse_sampled_dataset\u001b[38;5;241m.\u001b[39mX[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), \u001b[38;5;28mlen\u001b[39m(coarse_sampled_dataset\u001b[38;5;241m.\u001b[39mX[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m][\u001b[38;5;241m0\u001b[39m])) \u001b[38;5;66;03m# each row represent the sequence and each featuere represent the unique k-mers profile and the values represent the frequencies to it; exclude the last col as that is ambiguous\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(coarse_sampled_dataset\u001b[38;5;241m.\u001b[39mY))\n","\u001b[0;31mNameError\u001b[0m: name 'samples_500_index' is not defined"]}],"source":["# Example usage\n","#input_file_path = './training_data/train_raw_reads.fasta'\n","input_file_path = './training_data/train_6mers.npy'\n","\n","coarse_sampled_dataset = CS4220Dataset(data_file=input_file_path, label_df=coarse_label_df, samples_index=samples_500_index)\n","print(len(coarse_sampled_dataset.X[:, :-1]), len(coarse_sampled_dataset.X[:, :-1][0])) # each row represent the sequence and each featuere represent the unique k-mers profile and the values represent the frequencies to it; exclude the last col as that is ambiguous\n","print(len(coarse_sampled_dataset.Y))"]},{"cell_type":"code","execution_count":18,"metadata":{"id":"0gtQxTzP57JM"},"outputs":[{"name":"stdout","output_type":"stream","text":["20800 2080\n","20800\n"]}],"source":["# Example usage\n","#input_file_path = './training_data/train_raw_reads.fasta'\n","input_file_path = './training_data/train_6mers.npy'\n","\n","sampled_dataset = CS4220Dataset(data_file=input_file_path, label_df=processed_label_df, samples_index=samples_800_index)\n","print(len(sampled_dataset.X[:, :-1]), len(sampled_dataset.X[:, :-1][0])) # each row represent the sequence and each featuere represent the unique k-mers profile and the values represent the frequencies to it; exclude the last col as that is ambiguous\n","print(len(sampled_dataset.Y))"]},{"cell_type":"code","execution_count":21,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["461581 2080\n","461581\n"]}],"source":["# Using all data\n","all_dataset = CS4220Dataset(data_file=input_file_path, label_df=processed_label_df)\n","print(len(all_dataset.X[:, :-1]), len(all_dataset.X[:, :-1][0])) # each row represent the sequence and each featuere represent the unique k-mers profile and the values represent the frequencies to it; exclude the last col as that is ambiguous\n","print(len(all_dataset.Y))"]},{"cell_type":"markdown","metadata":{},"source":["## Individual Dataset Preparation"]},{"cell_type":"markdown","metadata":{},"source":["### Features by Std"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Getting all the low variation of mean kmer \n","STD_DEV_THRESHOLD = 0.0001\n","kmer_analysis = get_kmer_analysis_map(all_dataset)\n","\n","# For entire dataset\n","getting_no_kmer_existence(kmer_analysis)\n","std_accross_labels_sorted = get_std_across_labels_by_kmer(kmer_analysis)\n","\n","# Tunable parameter\n","all_dataset_features_to_keep = [key for key, elem in std_accross_labels_sorted.items() if elem >  STD_DEV_THRESHOLD]\n","\n","idx_keep_by_std_filter = all_dataset.X_mapped.columns.get_indexer(all_dataset_features_to_keep)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["array_by_std_filter = np.take(all_dataset.X, idx_keep_by_std_filter, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["array_by_std_filter.shape"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["array_by_feature_impt = np.take(all_dataset.X, idx_keep_by_feature_impt, axis=1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["std_accross_labels_sorted.keys()"]},{"cell_type":"markdown","metadata":{},"source":["### Features by Feature Importance"]},{"cell_type":"code","execution_count":24,"metadata":{},"outputs":[],"source":["# NUMBER\n","NUMBER_TO_DROP = 1000\n","feature_impt = pd.read_csv(\"/Users/melloo21/Desktop/NUS Items/CS4220/cs4220_project_2/cs4220-pathogen-detection/assets/feature_impt.csv\")\n","idx_keep_by_feature_impt = feature_impt.sort_values(by=\"importances\").iloc[NUMBER_TO_DROP:].index\n"]},{"cell_type":"code","execution_count":25,"metadata":{},"outputs":[],"source":["array_by_feature_impt = np.take(all_dataset.X, idx_keep_by_feature_impt, axis=1)"]},{"cell_type":"code","execution_count":26,"metadata":{},"outputs":[{"data":{"text/plain":["(461581, 1081)"]},"execution_count":26,"metadata":{},"output_type":"execute_result"}],"source":["array_by_feature_impt.shape"]},{"cell_type":"markdown","metadata":{},"source":["## Modelling"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# feature impt set \n","starting_time = timeit.default_timer()\n","\n","model = RandomForestRegressor(random_state=1, max_depth=16,verbose=1)\n","%timeit\n","# Train the model using the training sets\n","model.fit(array_by_feature_impt, all_dataset.Y)\n","dump(regr, 'models/rf_feature_impt_filtered.joblib')\n","print(\"Time taken :\", timeit.default_timer() - starting_time)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["performance_evaluate(\n","    train_dataset=(array_by_feature_impt, all_dataset.Y),\n","    valid_dataset=(array_by_feature_impt, all_dataset.Y),\n","    model_name=\"rf_feature_impt_filtered\",\n","    filepath=\"models\"\n",")\n","\n","all_jaccard_index, all_pred, all_true = get_all_jaccard_index_with_filter(model=regr, label_encoder=label_encoder,filtered_idx=idx_keep_by_feature_impt )\n","\n","print(['patient {}: {}'.format(c,item) for c, item in enumerate(all_jaccard_index)], 'avg: {}'.format(np.mean(all_jaccard_index)))"]},{"cell_type":"markdown","metadata":{"id":"WMEImbbx57JO"},"source":["The data and labels are then accessible via `sampled_dataset.X` and `sampled_dataset.Y`."]},{"cell_type":"markdown","metadata":{"id":"LqxSE5uV57JO"},"source":["## Quick Exploratory Data Analysis\n","\n","We can use a [UMAP plot](https://www.scdiscoveries.com/blog/knowledge/what-is-a-umap-plot/) to try to group reads in the same species together, according to their $k$-mer profiles."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rtFnX61T57JP","outputId":"6fc7344f-b8a8-433d-ec21-c1bbd22d8633"},"outputs":[],"source":["# the last column of our 6mer training dataset can be ignored\n","# UMAP aims to find a low dimensional representation of the data [https://umap-learn.readthedocs.io/en/latest/parameters.html]\n","# UMAP hyperparameters that impact the data -- n_neighbours, min_dist, n_components, metric\n","# n_neighbors: controls how UMAP balances local versus global structure -- low neighbors will cause UMAP to \n","# focus on very local structures -- (Default values : 15)\n","\n","# min_dist: (Default values: 0.1) -- controls how tightly UMAP is allowed to pack points together , lower value of min_dist will \n","# result in clumpier embeddings. This will be \n","\n","# n_component: (Default = 2) Number of dimensions, n=1 UMAP data will fall into a line\n","# metric: (Default)\n","mapper = UMAP(metric=\"euclidean\").fit(sampled_dataset.X[:, :-1])\n","\n","labels_tc = label_encoder.inverse_transform(sampled_dataset.Y)\n","ax = umap.plot.points(mapper, labels=labels_tc,  width=1500, height=1500)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Doing a UMAP for 2 coarse grained model\n","\n","mapper = UMAP(metric=\"euclidean\").fit(coarse_sampled_dataset.X[:, :-1])\n","\n","coarse_labels_tc = coarse_label_encoder.inverse_transform(coarse_sampled_dataset.Y)\n","ax = umap.plot.points(mapper, labels=coarse_labels_tc,  width=1500, height=1500)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# UMAP for the whole dataset\n","mapper = UMAP(metric=\"euclidean\").fit(all_dataset.X[:, :-1])\n","\n","labels_tc = le.inverse_transform(all_dataset.Y)\n","ax = umap.plot.points(mapper, labels=labels_tc,  width=1500, height=1500)"]},{"cell_type":"markdown","metadata":{},"source":["#### Just trying to integrate PCA into the UMAP to reduce the number of features and see if it cluster better but that doesn't seem to be the case "]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.decomposition import PCA\n","pca = PCA(n_components=2080)  # Use the maximum number of components\n","pca.fit(sampled_dataset.X[:, :-1])\n","cumulative_variance = np.cumsum(pca.explained_variance_ratio_)\n","plt.plot(range(1, len(cumulative_variance) + 1), cumulative_variance, marker='o', linestyle='-')\n","plt.xlabel('Number of Components')\n","plt.ylabel('Cumulative Explained Variance')\n","plt.title('Cumulative Explained Variance by Number of Components')\n","plt.grid(True)\n","plt.show()\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["n_components_95 = np.argmax(cumulative_variance >= 0.95) + 1\n","n_components_95"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["sampled_X=sampled_dataset.X[:, :-1]\n","pca = PCA(n_components=1060)  # Specify the number of components to keep\n","sampled_X_pca = pca.fit_transform(sampled_X)\n","mapper_pca = UMAP().fit(sampled_X_pca)\n","labels_tc_pca = le.inverse_transform(sampled_dataset.Y)\n","ax = umap.plot.points(mapper_pca, labels=labels_tc_pca,  width=1500, height=1500)"]},{"cell_type":"markdown","metadata":{"id":"oc7zBZRH57JQ"},"source":["What interesting patterns can you notice? Some observations might be:\n","- Some species are pretty separated in clusters.\n","- Even though most species lie in separate groups, we can see some regions where points from different species are overlapping.\n","- There are some \"lost points\" for pretty much every species.\n","\n","Do you think these pieces of information can help you in something? Think about it.\n","\n","Eugene's take on it:\n","- certain species have distinct genomic or sequence profiles that differentiate them from other species in the dataset\n","- the overlapping regions suggest shared genomic sequence between closely related species\n","- The \"lost points\" refer to data points that do not clearly belong to any specific cluster or group in the UMAP plot.\n","\n","\n","## Training\n","\n","OK, time for training. As baseline we will choose the simplest [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) model.\n","\n","You can fine-tune the logistic regression hyperparameters such as the penalty and regularization term.\n","\n","```python\n","from sklearn.model_selection import GridSearchCV\n","\n","#list of items to tune\n","parameters_lr = [{'penalty': ['l1','l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\n","\n","starting_time = timeit.default_timer()\n","\n","regr = LogisticRegression(random_state=2023, solver='saga', n_jobs=-1, class_weight='balanced', max_iter=50, verbose=1)\n","\n","grid_search_lr = GridSearchCV(estimator = regr,\n","                              param_grid = parameters_lr,\n","                              scoring = 'accuracy',\n","                              cv = 3,\n","                              n_jobs = -1)\n","\n","# the last column of our 6mer training dataset can be ignored (training labels)\n","grid_search_lr.fit(sampled_dataset.X, sampled_dataset.Y)\n","best_accuracy_lr = grid_search_lr.best_score_\n","best_parameter_lr = grid_search_lr.best_params_\n","\n","```\n","\n","This might take a long time. Here, we skip this step and jump straight to training."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split\n","# Having a training/validation set\n","X_train, X_test, y_train, y_test = train_test_split(sampled_dataset.X, sampled_dataset.Y,\n","                                                    stratify=sampled_dataset.Y, \n","                                                    test_size=0.1)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Using a smaller lightweight model\n","from sklearn.naive_bayes import ComplementNB\n","\n","clf = ComplementNB()\n","clf.fit(X_train, y_train)\n","\n","dump(clf, 'models/nb_model.joblib')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# linear classifier using scikit-learn\n","# Best Parameter of LR: {'C': 100, 'penalty': 'l1'}\n","\n","starting_time = timeit.default_timer()\n","\n","regr = LogisticRegression(C=100, penalty='l1', solver='saga', n_jobs=-1, class_weight='balanced', max_iter=100, verbose=1)\n","%timeit\n","# Train the model using the training sets\n","regr.fit(X_train, y_train)\n","dump(regr, 'models/log_reg.joblib')\n","print(\"Time taken :\", timeit.default_timer() - starting_time)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["performance_evaluate(\n","    train_dataset=(X_train, y_train),\n","    valid_dataset=(X_test,y_test),\n","    model_name=\"log_reg\",\n","    filepath=\"models\"\n",")\n","\n","all_jaccard_index, all_pred, all_true = get_all_jaccard_index(model=regr, label_encoder=label_encoder)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.mean(all_jaccard_index)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"AM8Tdla257JQ","outputId":"dedbd09d-2150-47a8-c35c-f8d85fb3bf99"},"outputs":[],"source":["# linear classifier using scikit-learn\n","# Best Parameter of LR: {'C': 100, 'penalty': 'l1'}\n","\n","starting_time = timeit.default_timer()\n","\n","regr = LogisticRegression(C=100, penalty='l1', solver='saga', n_jobs=-1, class_weight='balanced', max_iter=50, verbose=1)\n","%timeit\n","# Train the model using the training sets\n","regr.fit(sampled_dataset.X, sampled_dataset.Y)\n","print(\"Time taken :\", timeit.default_timer() - starting_time)\n","dump(regr, 'models/log_reg_orig.joblib')"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["performance_evaluate(\n","    train_dataset=(sampled_dataset.X, sampled_dataset.Y),\n","    valid_dataset=(sampled_dataset.X, sampled_dataset.Y),\n","    model_name=\"baseline\",\n","    filepath=\"models\"\n",")\n","\n","baseline_model =_read_model(filepath=\"models\", model_name=\"baseline\")\n","all_jaccard_index, all_pred, all_true = get_all_jaccard_index(model=baseline_model, label_encoder=label_encoder)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["all_jaccard_index"]},{"cell_type":"markdown","metadata":{"id":"SlQWHsVU57JR"},"source":["We can now save our trained model for later usage. This is an example of how you can send your model to our final evaluation."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9dcRzDgi57JS","outputId":"7b05ce5a-cad8-4823-a564-4305542b46b8"},"outputs":[],"source":["from joblib import dump, load\n","dump(regr, 'models/baseline.joblib')"]},{"cell_type":"markdown","metadata":{},"source":["### Trying out the GridSearchCV method he suggested but takes even longer and coef_ did not coverge as max_iter of 500 is reached"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import GridSearchCV\n","\n","#list of items to tune\n","parameters_lr = [{'penalty': ['l1','l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\n","\n","starting_time = timeit.default_timer()\n","\n","regr = LogisticRegression(random_state=2023, solver='saga', n_jobs=-1, class_weight='balanced', max_iter=500, verbose=1)\n","\n","grid_search_lr = GridSearchCV(estimator = regr,\n","                              param_grid = parameters_lr,\n","                              scoring = 'accuracy',\n","                              cv = 3,\n","                              n_jobs = -1)\n","\n","# the last column of our 6mer training dataset can be ignored (training labels)\n","grid_search_lr.fit(sampled_dataset.X, sampled_dataset.Y)\n","best_accuracy_lr = grid_search_lr.best_score_\n","best_parameter_lr = grid_search_lr.best_params_"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["np.random.random()"]},{"cell_type":"markdown","metadata":{"id":"LwWuRXTW57JS"},"source":["Then the model can be loaded using\n","\n","```python\n","# load trained model\n","regr = load('models/baseline.joblib')\n","```\n","\n","## Evaluation of Model\n","\n","Now that you have your trained model, you can use it on each of the patient's read dataset and try to find the pathogens that appear in each patient.\n","\n","Your model is evaluated based on [**Jaccard index**](https://en.wikipedia.org/wiki/Jaccard_index#Jaccard_index_in_binary_classification_confusion_matrices). For patient $i$, let $P$ be the set of pathogen species your model predicted (or $\\{\\text{decoy}\\}$ if there is no pathogens predicted), and $T$ the set of pathogen species the patient actually have (or $\\{\\text{decoy}\\}$ if there is no pathogens in the reads), the score for your model is\n","\n","$$\\text{Jaccard index}=\\frac{|P\\cap T|}{|P \\cup T|}$$"]},{"cell_type":"markdown","metadata":{"id":"Be0a5nLF57JU"},"source":["Going back to our model: since we are using logistic regression, our model will classify each read to the class that has the highest probability of having the read. If we report all the species that a read has been classified to, then we may end up with a lot of false positives (why?).\n","\n","One potential way to counter this is to define a threshold. Here I used 0.95: I only report a species if I am 95% confident that one read comes from that species. Let's see how well my model will behave..."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["baseline_model =_read_model(filepath=\"models\", model_name=\"baseline\")\n","all_jaccard_index, all_pred, all_true = get_all_jaccard_index(model=baseline_model, label_encoder=label_encoder)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lo8i9Tnj57JV","outputId":"44675bb5-09fb-424d-a4e9-0dfd1238c886"},"outputs":[],"source":["print(['patient {}: {}'.format(c,item) for c, item in enumerate(all_jaccard_index)], 'avg: {}'.format(np.mean(all_jaccard_index)))"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["set(all_true)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["set(all_pred)"]},{"cell_type":"markdown","metadata":{"id":"o5tEWYmN57JV"},"source":["So the overall score for my model is 0.45. Not a bad start, but still much room for improvement. You don't necessarily need to work on this baseline; this was just released as an example. Have fun!!!"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"base","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":0}
