{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import timeit\n",
    "import time\n",
    "import statistics \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle as pkl\n",
    "from umap.umap_ import UMAP\n",
    "from joblib import dump, load\n",
    "from itertools import chain\n",
    "from typing import Union, Any\n",
    "\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "# Non-linear classifiers\n",
    "from sklearn import svm # Should do with RBF or polynomial kernel\n",
    "from sklearn.multiclass import OneVsRestClassifier, OneVsOneClassifier \n",
    "# Note OVR -- sensitive to imbalanced dataset, OVO is less sensitive\n",
    "from sklearn.ensemble import RandomForestClassifier, VotingClassifier\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import (NeighborhoodComponentsAnalysis, KNeighborsClassifier)\n",
    "\n",
    "import datashader as ds\n",
    "import datashader.transfer_functions as tf\n",
    "import datashader.bundling as bd\n",
    "import colorcet\n",
    "import matplotlib.cm\n",
    "import bokeh.plotting as bpl\n",
    "import bokeh.transform as btr\n",
    "import holoviews as hv\n",
    "import holoviews.operation.datashader as hd\n",
    "\n",
    "import umap.plot\n",
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor,RandomForestClassifier\n",
    "\n",
    "def create_label_df(df):\n",
    "    # This is required for dataset function\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df['species_name'].unique())\n",
    "    y_index = le.transform(df['species_name'].values)\n",
    "    df['labels'] = y_index\n",
    "    print(f\"Unique labels {len(df['species_name'].unique())}\")\n",
    "\n",
    "    return df, le\n",
    "\n",
    "def create_new_dataset(labels:pd.DataFrame, dateset:Union[pd.DataFrame, np.array], filtered:Union[str,list]):\n",
    "    if isinstance(filtered, str):\n",
    "        filter_idx = labels[labels.species_name != filtered].index\n",
    "        print(f\" filtered for {filtered} : {labels.iloc[labels[labels.species_name != filtered].index].species_name.unique()}\")\n",
    "        final_labels = labels.copy().iloc[filter_idx]\n",
    "        final_dataset = dateset.copy().iloc[filter_idx]\n",
    "\n",
    "        return pd.concat([final_labels,final_dataset], axis=1)\n",
    "        \n",
    "def filtered_array(array, idx_to_keep):\n",
    "    return np.take(array, idx_to_keep, axis=1)\n",
    "  \n",
    "def get_all_jaccard_index_with_filter( filepath:str,model:Any, label_encoder:Any , filtered_idx:Any, num_patients:int=10, threshold:float=0.95):\n",
    "\n",
    "    all_jaccard_index = []\n",
    "    all_pred = []\n",
    "    all_true = []\n",
    "    for id in range(num_patients):\n",
    "        patient_id=f'patient{id}'\n",
    "        print('predicting for {}'.format(patient_id))\n",
    "\n",
    "        with open(f'{filepath}/{patient_id}_6mers.npy', 'rb') as read_file:\n",
    "            df_test = np.load(read_file)\n",
    "\n",
    "        # regr.predict relies on argmax, thus predict to every single read and you will end up with many false positives\n",
    "        transformed_data = filtered_array(df_test,filtered_idx)\n",
    "        print(f\"Shape of {transformed_data.shape}\")\n",
    "        y_pred = model.predict(transformed_data)\n",
    "\n",
    "        # we can use regr.predict_proba to find a good threshold and predict only for case where the model is confident.\n",
    "        # here I apply 0.95 as the cutoff for my predictions, let's see how well my model will behave...\n",
    "        y_predprob = model.predict_proba(transformed_data)\n",
    "\n",
    "        # we get only predictions larger than the threshold and if there is more than one, we take the argmax again\n",
    "        final_predictions = label_encoder.inverse_transform(\n",
    "                                np.unique([np.argmax(item) for item in y_predprob if len(np.where(item >= threshold)[0]) >= 1]\n",
    "                            ))\n",
    "\n",
    "        # my pathogens dectected, decoy will be ignored\n",
    "        final_predictions = [item for item in final_predictions if item !='decoy']\n",
    "\n",
    "\n",
    "        ji, pred_pathogen, true_pathogen = jaccard_index_per_patient(filepath, patient_id, final_predictions)\n",
    "        print('Jaccard index: {}'.format(ji))\n",
    "        all_jaccard_index.append(ji)    \n",
    "        all_pred.append(pred_pathogen)\n",
    "        all_true.append(true_pathogen)\n",
    "\n",
    "    return all_jaccard_index, flatten(all_pred), flatten(all_true) \n",
    "\n",
    "def print_full(x):\n",
    "    pd.set_option('display.max_rows', len(x))\n",
    "    print(x)\n",
    "    pd.reset_option('display.max_rows')\n",
    "\n",
    "def random_forest_selector(X,y):\n",
    "    # Using feature importance to select features\n",
    "    model = RandomForestClassifier(random_state=1, max_depth=10)\n",
    "    model.fit(X,y)\n",
    "    features = X.columns\n",
    "    importances = model.feature_importances_\n",
    "    # Create a dataframe for feature importance\n",
    "    feature_importance_df = pd.DataFrame({\"features\": list(features) ,\"importances\": list(importances)} )\n",
    "    indices = np.argsort(importances)[-9:]  # top 10 features\n",
    "    plt.title('Feature Importances')\n",
    "    plt.barh(range(len(indices)), importances[indices], color='b', align='center')\n",
    "    plt.yticks(range(len(indices)), [features[i] for i in indices])\n",
    "    plt.xlabel('Relative Importance')\n",
    "    plt.show()\n",
    "\n",
    "    return feature_importance_df\n",
    "\n",
    "def create_coarse_labels(df):\n",
    "    df['coarse_species_name'] = np.where(df['species_name'] != \"decoy\", \"non_decoy\", \"decoy\")\n",
    "    le = preprocessing.LabelEncoder()\n",
    "    le.fit(df['coarse_species_name'].unique())\n",
    "    y_index = le.transform(df['coarse_species_name'].values)\n",
    "    df['labels'] = y_index\n",
    "    print(f\"Unique labels {len(df['coarse_species_name'].unique())}\")\n",
    "    return df, le\n",
    "\n",
    "\n",
    "\n",
    "def get_all_jaccard_index_with_transformation( filepath:str, model:Any, label_encoder:Any , x_transformer:Any, num_patients:int=10, threshold:float=0.95):\n",
    "\n",
    "    all_jaccard_index = []\n",
    "    all_pred = []\n",
    "    all_true = []\n",
    "    for id in range(num_patients):\n",
    "        patient_id=f'patient{id}'\n",
    "        print('predicting for {}'.format(patient_id))\n",
    "        with open(f'{filepath}/{patient_id}_6mers.npy', 'rb') as read_file:\n",
    "            df_test = np.load(read_file)\n",
    "\n",
    "        # regr.predict relies on argmax, thus predict to every single read and you will end up with many false positives\n",
    "        transformed_data =x_transformer.fit_transform(df_test)\n",
    "        y_pred = model.predict(transformed_data)\n",
    "\n",
    "        # we can use regr.predict_proba to find a good threshold and predict only for case where the model is confident.\n",
    "        # here I apply 0.95 as the cutoff for my predictions, let's see how well my model will behave...\n",
    "        y_predprob = model.predict_proba(transformed_data)\n",
    "\n",
    "        # we get only predictions larger than the threshold and if there is more than one, we take the argmax again\n",
    "        final_predictions = label_encoder.inverse_transform(\n",
    "                                np.unique([np.argmax(item) for item in y_predprob if len(np.where(item >= threshold)[0]) >= 1]\n",
    "                            ))\n",
    "        \n",
    "        # my pathogens dectected, decoy will be ignored\n",
    "        final_predictions = [item for item in final_predictions if item !='decoy']\n",
    "\n",
    "        print(f\"final_predictions {final_predictions} , unique {np.unique(final_predictions)}\")\n",
    "        ji, pred_pathogen, true_pathogen = jaccard_index_per_patient(filepath, patient_id, final_predictions)\n",
    "        print('Jaccard index: {}'.format(ji))\n",
    "        all_jaccard_index.append(ji)    \n",
    "        all_pred.append(pred_pathogen)\n",
    "        all_true.append(true_pathogen)\n",
    "\n",
    "    return all_jaccard_index, flatten(all_pred), flatten(all_true)\n",
    "\n",
    "def jaccard_index_per_patient(filepath:str,fname:str, preds):\n",
    "    # Generic filename\n",
    "    print(f\"{filepath}/{fname}_labels.csv\")\n",
    "    df_true = pd.read_csv(f'{filepath}/{fname}_labels.csv')\n",
    "    tp, fp, tp_fn = 0, 0, df_true['labels'].shape[0]\n",
    "    print('my predition(s) for patient {}:'.format(fname))\n",
    "    print(preds)\n",
    "    print('true pathogen')\n",
    "    print(df_true['labels'].values)\n",
    "    # if don't predict any pathogen, it means there is only decoy in the test dataset (your prediction)\n",
    "    if len(preds) == 0:\n",
    "        preds = ['decoy']\n",
    "    for item in np.unique(preds):\n",
    "        if item in df_true['labels'].values:\n",
    "            tp += 1\n",
    "        else:\n",
    "            fp += 1\n",
    "    #you have to predict all labels correctly, but you are penalized for any false positive\n",
    "    return tp / (tp_fn + fp) , preds, df_true['labels'].values\n",
    "\n",
    "\n",
    "\n",
    "def predict_kmer_profile(filepath:str, fname: str, model:Any, label_encoder:Any , filtered_idx:Any, threshold:float=0.95): #expecet the fname to be of npy file\n",
    "    # all_jaccard_index = []\n",
    "    # all_pred = []\n",
    "    # all_true = []\n",
    "\n",
    "    f_id, suffix_=fname.split('_')\n",
    "    print('predicting for {}'.format(f_id))\n",
    "    \n",
    "    with open(f'{filepath}/{fname}', 'rb') as read_file:\n",
    "        df_test = np.load(read_file)\n",
    "\n",
    "    # regr.predict relies on argmax, thus predict to every single read and you will end up with many false positives\n",
    "    transformed_data = filtered_array(df_test,filtered_idx)\n",
    "    print(f\"Shape of {transformed_data.shape}\")\n",
    "    y_pred = model.predict(transformed_data)\n",
    "\n",
    "    # we can use regr.predict_proba to find a good threshold and predict only for case where the model is confident.\n",
    "    # here I apply 0.95 as the cutoff for my predictions, let's see how well my model will behave...\n",
    "    y_predprob = model.predict_proba(transformed_data)\n",
    "\n",
    "    # we get only predictions larger than the threshold and if there is more than one, we take the argmax again\n",
    "    final_predictions = label_encoder.inverse_transform(\n",
    "                            np.unique([np.argmax(item) for item in y_predprob if len(np.where(item >= threshold)[0]) >= 1]\n",
    "                        ))\n",
    "    \n",
    "    # my pathogens dectected, decoy will be ignored\n",
    "    final_predictions = [item for item in final_predictions if item !='decoy']\n",
    "\n",
    "    print(f\"final_predictions {final_predictions} , unique {np.unique(final_predictions)}\")\n",
    "    ji, pred_pathogen, true_pathogen = jaccard_index_per_patient(filepath, f_id, final_predictions)\n",
    "    print('Jaccard index: {}'.format(ji))\n",
    "    # all_jaccard_index.append(ji)    \n",
    "    # all_pred.append(pred_pathogen)\n",
    "    # all_true.append(true_pathogen)\n",
    "    data_={'labels':pred_pathogen}\n",
    "    df=pd.DataFrame(data=data_)\n",
    "    df.to_csv(f\"{f_id}_prediction.csv\", index=False)\n",
    "\n",
    "def flatten(original_list:list):\n",
    "    return list(chain.from_iterable(original_list))\n",
    "\n",
    "def get_kmer_analysis_map(dataset):\n",
    "    test = dataset.X_mapped.copy()\n",
    "    test[\"labels\"] = dataset.Y   \n",
    "    return test.groupby(\"labels\").mean().reset_index()\n",
    "    \n",
    "def getting_no_kmer_existence(analysis):\n",
    "\n",
    "    kmer_by_label = dict()\n",
    "    # Obtaining the profile\n",
    "    for elem in analysis.index:\n",
    "        kmer_by_label[elem] = analysis.iloc[elem,1:-1]\n",
    "\n",
    "    # Getting label without that kmer\n",
    "    for elem in analysis.index:\n",
    "        kmer_zero = list(kmer_by_label[elem][kmer_by_label[elem]==0].index)\n",
    "        if (len(kmer_zero) > 0):\n",
    "            print(f\" label {elem} ::  {kmer_zero}\")\n",
    "\n",
    "    return kmer_zero\n",
    "\n",
    "def get_label_by_kmer(kmer_analysis):\n",
    "    label_profile_by_kmer = dict()\n",
    "\n",
    "    # Obtaining the profile\n",
    "    for elem in range(1,len(kmer_analysis.columns)-1):\n",
    "        label_profile_by_kmer[kmer_analysis.columns[elem]] = kmer_analysis.iloc[:,elem]\n",
    "    return label_profile_by_kmer\n",
    "\n",
    "def get_std_across_labels_by_kmer(kmer_analysis):\n",
    "    label_profile_by_kmer = get_label_by_kmer(kmer_analysis)\n",
    "\n",
    "    std_accross_labels = dict()\n",
    "\n",
    "    for key, values in label_profile_by_kmer.items():\n",
    "        std_accross_labels[key] = np.std(values)\n",
    "\n",
    "    # Sort by variation\n",
    "    std_accross_labels_sorted = dict(sorted(std_accross_labels.items(), key=lambda item: item[1]))\n",
    "\n",
    "    return std_accross_labels_sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting for patient0\n"
     ]
    }
   ],
   "source": [
    "fname='patient0_6mers.npy'\n",
    "\n",
    "f_id, suffix_=fname.split('_')\n",
    "print('predicting for {}'.format(f_id))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique labels 26\n",
      "1580\n"
     ]
    }
   ],
   "source": [
    "# LABEL_PATH = 'train_labels.csv'\n",
    "LABEL_PATH = 'training_data/train_labels.csv'\n",
    "\n",
    "processed_label_df ,label_encoder = create_label_df(pd.read_csv(LABEL_PATH))\n",
    "\n",
    "# NUMBER\n",
    "NUMBER_TO_DROP = 501\n",
    "# feature_impt = pd.read_csv(\"feature_impt.csv\")\n",
    "feature_impt = pd.read_csv(\"assets/feature_impt.csv\")\n",
    "idx_keep_by_feature_impt_1580= feature_impt.sort_values(by=\"importances\").iloc[NUMBER_TO_DROP:].index\n",
    "print(len(idx_keep_by_feature_impt_1580))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting for patient0\n",
      "Shape of (10054, 1580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data/patient0_labels.csv\n",
      "my predition(s) for patient patient0:\n",
      "['staphylococcus_aureus']\n",
      "true pathogen\n",
      "['staphylococcus_aureus']\n",
      "Jaccard index: 1.0\n",
      "predicting for patient1\n",
      "Shape of (10132, 1580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.5s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data/patient1_labels.csv\n",
      "my predition(s) for patient patient1:\n",
      "['neisseria_gonorrhoeae']\n",
      "true pathogen\n",
      "['staphylococcus_pyogenes']\n",
      "Jaccard index: 0.0\n",
      "predicting for patient2\n",
      "Shape of (10022, 1580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data/patient2_labels.csv\n",
      "my predition(s) for patient patient2:\n",
      "['burkholderia_pseudomallei', 'corynebacterium_ulcerans']\n",
      "true pathogen\n",
      "['burkholderia_pseudomallei' 'corynebacterium_ulcerans']\n",
      "Jaccard index: 1.0\n",
      "predicting for patient3\n",
      "Shape of (9984, 1580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data/patient3_labels.csv\n",
      "my predition(s) for patient patient3:\n",
      "['pseudomonas_aeruginosa']\n",
      "true pathogen\n",
      "['pseudomonas_aeruginosa']\n",
      "Jaccard index: 1.0\n",
      "predicting for patient4\n",
      "Shape of (10086, 1580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data/patient4_labels.csv\n",
      "my predition(s) for patient patient4:\n",
      "['corynebacterium_diphtheriae']\n",
      "true pathogen\n",
      "['corynebacterium_diphtheriae']\n",
      "Jaccard index: 1.0\n",
      "predicting for patient5\n",
      "Shape of (10046, 1580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data/patient5_labels.csv\n",
      "my predition(s) for patient patient5:\n",
      "['streptococcus_pneumoniae']\n",
      "true pathogen\n",
      "['streptococcus_pneumoniae']\n",
      "Jaccard index: 1.0\n",
      "predicting for patient6\n",
      "Shape of (9974, 1580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.5s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data/patient6_labels.csv\n",
      "my predition(s) for patient patient6:\n",
      "['mycobacterium_ulcerans']\n",
      "true pathogen\n",
      "['mycobacterium_ulcerans']\n",
      "Jaccard index: 1.0\n",
      "predicting for patient7\n",
      "Shape of (10046, 1580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data/patient7_labels.csv\n",
      "my predition(s) for patient patient7:\n",
      "['neisseria_gonorrhoeae']\n",
      "true pathogen\n",
      "['mycobacterium_tuberculosis' 'streptococcus_pneumoniae']\n",
      "Jaccard index: 0.0\n",
      "predicting for patient8\n",
      "Shape of (10009, 1580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data/patient8_labels.csv\n",
      "my predition(s) for patient patient8:\n",
      "['streptococcus_pneumoniae']\n",
      "true pathogen\n",
      "['streptococcus_pneumoniae']\n",
      "Jaccard index: 1.0\n",
      "predicting for patient9\n",
      "Shape of (10074, 1580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test_data/patient9_labels.csv\n",
      "my predition(s) for patient patient9:\n",
      "['burkholderia_pseudomallei', 'neisseria_gonorrhoeae']\n",
      "true pathogen\n",
      "['burkholderia_pseudomallei']\n",
      "Jaccard index: 0.5\n",
      "['patient 0: 1.0', 'patient 1: 0.0', 'patient 2: 1.0', 'patient 3: 1.0', 'patient 4: 1.0', 'patient 5: 1.0', 'patient 6: 1.0', 'patient 7: 0.0', 'patient 8: 1.0', 'patient 9: 0.5'] avg: 0.75\n"
     ]
    }
   ],
   "source": [
    "# MODEL_PATH= \"rf_feature_impt_filtered_1580.joblib\" # Download in Gdrive\n",
    "MODEL_PATH= \"models/rf_feature_impt_filtered_1580.joblib\" # Download in Gdrive\n",
    "\n",
    "model = load(MODEL_PATH)\n",
    "\n",
    "all_jaccard_index, all_pred, all_true = get_all_jaccard_index_with_filter(\n",
    "    filepath=r\"test_data\", # specify filepath name where test data is in\n",
    "    model=model, # specify model name\n",
    "    label_encoder=label_encoder, # specify label encoder\n",
    "    filtered_idx=idx_keep_by_feature_impt_1580, \n",
    "    threshold=0.6)\n",
    "\n",
    "print(['patient {}: {}'.format(c,item) for c, item in enumerate(all_jaccard_index)], 'avg: {}'.format(np.mean(all_jaccard_index)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicting for patient0\n",
      "Shape of (10054, 1580)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n",
      "[Parallel(n_jobs=1)]: Done  49 tasks      | elapsed:    0.1s\n",
      "[Parallel(n_jobs=1)]: Done 199 tasks      | elapsed:    0.4s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "final_predictions ['staphylococcus_aureus'] , unique ['staphylococcus_aureus']\n",
      "test_data/patient0_labels.csv\n",
      "my predition(s) for patient patient0:\n",
      "['staphylococcus_aureus']\n",
      "true pathogen\n",
      "['staphylococcus_aureus']\n",
      "Jaccard index: 1.0\n"
     ]
    }
   ],
   "source": [
    "predict_kmer_profile(\n",
    "    filepath=r\"test_data\", # specify filepath name where test data is in\n",
    "    fname=\"patient0_6mers.npy\",\n",
    "    model=model, # specify model name\n",
    "    label_encoder=label_encoder, # specify label encoder\n",
    "    filtered_idx=idx_keep_by_feature_impt_1580, \n",
    "    threshold=0.6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cs4220_pathogen_detection",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
