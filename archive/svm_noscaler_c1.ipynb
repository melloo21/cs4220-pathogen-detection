{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"6T_tuslc57JA"},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/melloo21/opt/anaconda3/envs/cs4220_pathogen_detection/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","/Users/melloo21/opt/anaconda3/envs/cs4220_pathogen_detection/lib/python3.8/site-packages/umap/plot.py:203: NumbaDeprecationWarning: The keyword argument 'nopython=False' was supplied. From Numba 0.59.0 the default is being changed to True and use of 'nopython=False' will raise a warning as the argument will have no effect. See https://numba.readthedocs.io/en/stable/reference/deprecation.html#deprecation-of-object-mode-fall-back-behaviour-when-using-jit for details.\n","  @numba.jit(nopython=False)\n"]}],"source":["# import packages\n","from joblib import dump, load\n","import numpy as np\n","import pandas as pd\n","import timeit\n","import time\n","from sklearn import preprocessing\n","import matplotlib\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import pickle as pkl\n","from umap.umap_ import UMAP\n","import umap.plot\n","\n","from sklearn.svm import SVC\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.preprocessing import StandardScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.multiclass import OneVsRestClassifier"]},{"cell_type":"markdown","metadata":{"id":"FJtaUfX_57JE"},"source":["## Parsing Input Data\n","\n","First, some dataset statistics. We load our training labels (ground truth) and see how many reads a category (species) have."]},{"cell_type":"code","execution_count":2,"metadata":{"id":"H2nDX_MC57JF","outputId":"5184abfb-4fad-45ef-b37d-a121e374f81a"},"outputs":[{"data":{"text/plain":["species_name\n","decoy                              413476\n","burkholderia_pseudomallei            3533\n","pseudomonas_aeruginosa               3126\n","klebsiella_michiganensis             2989\n","mycobacterium_ulcerans               2910\n","klebsiella_pneumoniae                2806\n","serratia_liquefaciens                2629\n","vibrio_parahaemolyticus              2579\n","salmonella_enterica_typhimurium      2507\n","yersinia_enterocolitica              2276\n","stenotrophomonas_maltophilia         2217\n","mycobacterium_tuberculosis           2175\n","clostridioides_difficile             2007\n","acinetobacter_baumannii              1964\n","legionella_pneumophila               1753\n","listeria_monocytogenes               1479\n","staphylococcus_aureus                1384\n","staphylococcus_pseudintermedius      1328\n","corynebacterium_ulcerans             1266\n","corynebacterium_diphtheriae          1194\n","streptococcus_suis                   1092\n","neisseria_gonorrhoeae                1087\n","streptococcus_agalactiae             1060\n","streptococcus_pneumoniae             1056\n","staphylococcus_pyogenes               856\n","campylobacter_jejuni                  832\n","Name: count, dtype: int64"]},"execution_count":2,"metadata":{},"output_type":"execute_result"}],"source":["label_df = pd.read_csv('./training_data/train_labels.csv')\n","label_df['species_name'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"CLk96Igt57JG"},"source":["It seems that we have a lot of decoy reads (decoy means sequencing reads from human or commensal species)."]},{"cell_type":"code","execution_count":3,"metadata":{"id":"-0pyqyW-57JH"},"outputs":[],"source":["# snippet to load the grouth truth training labels and normalize the label predictions.\n","# your trained model will predict in this space (26 classes - pathogens and decoy)\n","\n","le = preprocessing.LabelEncoder()\n","le.fit(label_df['species_name'].unique())\n","y_index = le.transform(label_df['species_name'].values)\n","label_df['labels'] = y_index"]},{"cell_type":"markdown","metadata":{"id":"4Zg_NzrW57JK"},"source":["In this project, we try to use canonical $k$-mer profiles to represent each read in the input. Here, we are using $k=6$ and consequently 2081 features (including 1 feature `IGNORE` for ambiguous $k$-mer) for each read. Read [this reference](https://bioinfologics.github.io/post/2018/09/17/k-mer-counting-part-i-introduction/) for more information.\n","\n","To help you save time, we implemented a utility class `CS4220Dataset` that can\n","- take in raw reads as input (`.fasta`, `.fa` files), and turn them into $k$-mer profiles, or\n","- take in $k$-mer profile as input (`.npy` files),\n","- allow you to sample data or create $k$-mer profile on the fly (during training) to save memory."]},{"cell_type":"code","execution_count":4,"metadata":{"id":"yL8keUan57JL"},"outputs":[],"source":["# Load dictionary that maps k-mer to their corresponding index.\n","# A k-mer and its reverse complement are mapped to the same index.\n","\n","import json\n","\n","with open(\"./training_data/6-mers.json\", 'r') as dict_file:\n","    canonical_kmer_dict = json.load(dict_file)"]},{"cell_type":"code","execution_count":5,"metadata":{"id":"2RS2EOwk57JL"},"outputs":[],"source":["# We define a utility function here that turns sequences to their 6-mer profiles.\n","\n","def sequence_to_kmer_profile(sequence : str, k : int = 6):\n","    \"\"\"\n","    Return the k-mer profile of the input sequence (string)\n","    \"\"\"\n","    res = np.zeros(len(set(canonical_kmer_dict.values())))\n","    for i in range(len(sequence) - k + 1):\n","        k_mer = sequence[i:i + k]\n","        if k_mer in canonical_kmer_dict:\n","            res[canonical_kmer_dict[k_mer]] += 1\n","        else:\n","            res[-1] += 1\n","\n","    res /= np.sum(res)\n","    return res"]},{"cell_type":"code","execution_count":6,"metadata":{"id":"0gtQxTzP57JM"},"outputs":[],"source":["import torch\n","from tqdm import tqdm\n","from torch.utils.data import Dataset, DataLoader\n","\n","class CS4220Dataset(Dataset):\n","    def __init__(self, data_file, label_df=None, k=6, samples_index=None, kmer_profile_on_the_fly=False, dtype=np.float32):\n","        \"\"\"\n","        Dataset class to load large CS4220 sequence database.\n","\n","        Args:\n","            - data_file (`str`): Can either be a *.fasta file if the input is raw reads, or *.npy file\n","                                 if the input is k-mer profile.\n","            - label_df (`pd.DataFrame` or `None`): A dataframe with \"labels\" column indicating the label\n","                                                   of the data (must match with data_file), or `None` if there is\n","                                                   no label (in the case of test sets).\n","            - k (`int`): The lengt of k-mer. We use 6 in this project.\n","            - samples_index (`List` or `None`): list of indices of data we sample from the data file. You\n","                                                can use this if the dataset is very large and can't fit in memory.\n","                                                set this to `None` if you want to use all the data.\n","            - kmer_profile_on_the_fly (`bool`): If input data_file is raw reads and this set to `True`,\n","                                                we will build k-mer profile on the fly. This is helpful if you want to\n","                                                alter the input sequences during training, or the k-mer profile can't fit in memory.\n","                                                Otherwise, we build k-mer profile in advance, which will speed up the\n","                                                training process.\n","            - dtype: type to store the k-mer profile. You may use, for example, `np.float32` for better precision,\n","                     or `np.float16` for smaller memory usage. If loaded from \".npy\" file, it is always `np.float16`.\n","        \"\"\"\n","        self.data_file = data_file\n","\n","        if \".fasta\" in data_file or \".fa\" in data_file or \".fna\" in data_file:\n","            self.is_raw_reads = True\n","        elif \".npy\" in data_file:\n","            self.is_raw_reads = False\n","        else:\n","            raise TypeError(f\"The input file must be either a fasta file containing raw reads (.fasta, .fa, .fna) or a numpy file containing k-mer profiles (.npy).\")\n","\n","\n","        self.label_df = label_df\n","        self.kmer_profile_otf = kmer_profile_on_the_fly\n","\n","        # k-mer length, set to be 6.\n","        self.k = k\n","\n","        # the samples we take from the read dataset\n","        self.samples_index = samples_index\n","\n","        self.dtype = dtype\n","\n","        # Load the data and store in self.reads and self.labels\n","        self.X = []\n","        self.Y = []\n","        self._read_labels()\n","        self._read_data()\n","\n","\n","    def _read_labels(self):\n","        \"\"\"\n","        Read the labels and record them in self.labels.\n","        \"\"\"\n","        if self.label_df is None:\n","            self.Y = None\n","        elif self.samples_index is None:\n","            # Load the whole dataset\n","            self.Y = list(self.label_df[\"labels\"])\n","        else:\n","            # Load only the data corresponding to the sampled index\n","            self.Y = list(self.label_df.iloc[self.samples_index][\"labels\"])\n","\n","\n","    def _read_data(self):\n","        if self.is_raw_reads:\n","            # Read the fasta file\n","            with open(self.data_file, 'r') as fasta_file:\n","                lines = fasta_file.readlines()\n","\n","            read_range = self.samples_index if self.samples_index is not None else range(int(len(lines) / 2))\n","            if not self.kmer_profile_otf:\n","                self.X = np.zeros(\n","                    (len(read_range), len(set(canonical_kmer_dict.values()))),\n","                    dtype=self.dtype\n","                )\n","\n","            for i, j in enumerate(tqdm(read_range, desc=f\"Parsing fasta file {self.data_file}\")):\n","                read = lines[j * 2 + 1].strip()\n","                if self.kmer_profile_otf:\n","                    # If chose to do k-mer profiling on the fly, simply store the reads\n","                    self.X.append(read)\n","                else:\n","                    # Otherwise, do k-mer profiling during training/testing, cost more time during training/testing\n","                    self.X[i, :] = sequence_to_kmer_profile(read, self.k)\n","        else:\n","            # Read the .npy file, and load the numpy matrix\n","            # Each row corresponds to a read, and each column corresponds to a k-mer (see training_data/6-mers.txt).\n","            self.X = np.load(self.data_file)\n","            if self.samples_index is not None:\n","                self.X = self.X[self.samples_index, :]\n","\n","    def __len__(self):\n","        return len(self.X)\n","\n","    def __getitem__(self, idx):\n","        \"\"\"\n","        If you are using pytorch, this function helps taking data points during each epoch\n","        of your training.\n","        \"\"\"\n","        x = self.X[idx]\n","        if self.kmer_profile_otf:\n","            read_tensor = torch.tensor(sequence_to_kmer_profile(x, self.k), dtype=self.dtype)\n","        else:\n","            read_tensor = torch.tensor(x)\n","\n","        label = self.Y[idx] if self.Y is not None else None\n","        return read_tensor, label\n"]},{"cell_type":"markdown","metadata":{"id":"LqxSE5uV57JO"},"source":["## Quick Exploratory Data Analysis\n","\n","We can use a [UMAP plot](https://www.scdiscoveries.com/blog/knowledge/what-is-a-umap-plot/) to try to group reads in the same species together, according to their $k$-mer profiles."]},{"cell_type":"markdown","metadata":{"id":"oc7zBZRH57JQ"},"source":["What interesting patterns can you notice? Some observations might be:\n","- Some species are pretty separated in clusters.\n","- Even though most species lie in separate groups, we can see some regions where points from different species are overlapping.\n","- There are some \"lost points\" for pretty much every species.\n","\n","Do you think these pieces of information can help you in something? Think about it.\n","\n","## Training\n","\n","OK, time for training. As baseline we will choose the simplest [logistic regression](https://en.wikipedia.org/wiki/Logistic_regression) model.\n","\n","You can fine-tune the logistic regression hyperparameters such as the penalty and regularization term.\n","\n","```python\n","from sklearn.model_selection import GridSearchCV\n","\n","#list of items to tune\n","parameters_lr = [{'penalty': ['l1','l2'], 'C': [0.001, 0.01, 0.1, 1, 10, 100]}]\n","\n","starting_time = timeit.default_timer()\n","\n","regr = LogisticRegression(random_state=2023, solver='saga', n_jobs=-1, class_weight='balanced', max_iter=50, verbose=1)\n","\n","grid_search_lr = GridSearchCV(estimator = regr,\n","                              param_grid = parameters_lr,\n","                              scoring = 'accuracy',\n","                              cv = 3,\n","                              n_jobs = -1)\n","\n","# the last column of our 6mer training dataset can be ignored (training labels)\n","grid_search_lr.fit(sampled_dataset.X, sampled_dataset.Y)\n","best_accuracy_lr = grid_search_lr.best_score_\n","best_parameter_lr = grid_search_lr.best_params_\n","\n","```\n","\n","This might take a long time. Here, we skip this step and jump straight to training."]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[],"source":["# samples_index = label_df.groupby('labels').sample(100, replace=True).index\n","# threshold=0.7\n","# input_file_path = './training_data/train_6mers.npy'\n","# dataset = CS4220Dataset(input_file_path, label_df, samples_index=samples_index)\n","# data = dataset.X\n","# labels = dataset.Y\n","\n","# X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n","\n","# scaler = StandardScaler()\n","# X_train_scaled = scaler.fit_transform(X_train)\n","# X_test_scaled = scaler.transform(X_test)\n","# model = SVC(kernel='rbf', C=100.0, probability=True)\n","# ovr = OneVsRestClassifier(model)\n","# ovr.fit(X_train_scaled, y_train)\n","# pred_prob = ovr.predict_proba(X_test_scaled)\n","\n","# final_predictions = le.inverse_transform(\n","#                         np.unique([np.argmax(item) for item in pred_prob if len(np.where(item >= threshold)[0]) >= 1]\n","#                     ))\n","# final_predictions"]},{"cell_type":"markdown","metadata":{},"source":["### Check on sampled data"]},{"cell_type":"code","execution_count":8,"metadata":{},"outputs":[],"source":["# samples_index = label_df.groupby('labels').sample(800, replace=False).index\n","\n","# input_file_path = './training_data/train_6mers.npy'\n","# dataset = CS4220Dataset(input_file_path, label_df, samples_index=samples_index)\n","# data = dataset.X\n","# labels = dataset.Y\n","\n","# X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n","\n","# scaler = StandardScaler()\n","# X_train_scaled = scaler.fit_transform(X_train)\n","# X_test_scaled = scaler.transform(X_test)\n","\n","# num_classes = len(set(labels))\n","# threshold = 0.9\n","\n","# # Define the parameter grid\n","# param_grid = {'estimator__C': [0.1, 1, 10, 100], 'estimator__kernel': ['linear', 'rbf', 'poly']}\n","\n","# # Initialize SVM classifier\n","# model = OneVsRestClassifier(SVC(probability=True))\n","\n","# # Perform grid search with cross-validation\n","# grid_search = GridSearchCV(model, param_grid, cv=5, verbose=2)\n","# search_result = grid_search.fit(X_train_scaled, y_train)\n","\n","# # Print the best parameters and the corresponding accuracy\n","# print(\"Best Parameters:\", grid_search.best_params_)\n","# print(\"Best Accuracy:\", grid_search.best_score_)\n"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["# means = search_result.cv_results_['mean_test_score']\n","# params = search_result.cv_results_['params']\n","\n","# for mean, param in zip(means, params):\n","#     print(\"%f with: %r\" % (mean, param))\n","\n","\n","# # Store the parameters of the best model\n","# best_params = model.best_params_"]},{"cell_type":"code","execution_count":10,"metadata":{},"outputs":[],"source":["# best_kernel = 'rbf'\n","# best_C = 10\n","# X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n","\n","# scaler = StandardScaler()\n","# X_train_scaled = scaler.fit_transform(X_train)\n","# X_test_scaled = scaler.transform(X_test)\n","\n","# threshold = 0.7\n","# model = SVC(kernel=best_kernel, C=best_c)\n","# model.fit(X_train_scaled, y_train)\n","\n","# pred = model.predict(X_test_scaled)\n","# pred_prob = model.predict_proba(X_test_scaled)\n","# final_predictions = le.inverse_transform(\n","#                         np.unique([np.argmax(item) for item in pred_prob if len(np.where(item >= threshold)[0]) >= 1]\n","#                     ))"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[],"source":["# split into train and test\n","# samples_index = label_df.groupby('labels').sample(800, replace=True).index\n","\n","# input_file_path = './training_data/train_6mers.npy'\n","# dataset = CS4220Dataset(input_file_path, label_df, samples_index=samples_index)\n","# data = dataset.X\n","# labels = dataset.Y\n","\n","# best_kernel = 'rbf'\n","# best_C = 100\n","# X_train, X_test, y_train, y_test = train_test_split(data, labels, test_size=0.2)\n","\n","# scaler = StandardScaler()\n","# X_train_scaled = scaler.fit_transform(X_train)\n","# X_test_scaled = scaler.transform(X_test)\n","\n","# model = SVC(kernel=best_kernel, C=best_C)\n","# model.fit(X_train_scaled, y_train)\n","# pred_prob = ovr.predict_proba(X_test_scaled)"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[{"data":{"text/html":["<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>OneVsRestClassifier(estimator=SVC(C=1, probability=True))</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" ><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">OneVsRestClassifier</label><div class=\"sk-toggleable__content\"><pre>OneVsRestClassifier(estimator=SVC(C=1, probability=True))</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" ><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, probability=True)</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-3\" type=\"checkbox\" ><label for=\"sk-estimator-id-3\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">SVC</label><div class=\"sk-toggleable__content\"><pre>SVC(C=1, probability=True)</pre></div></div></div></div></div></div></div></div></div></div>"],"text/plain":["OneVsRestClassifier(estimator=SVC(C=1, probability=True))"]},"execution_count":12,"metadata":{},"output_type":"execute_result"}],"source":["# run on all train\n","samples_index = label_df.groupby('labels').sample(800, replace=True).index\n","\n","input_file_path = './training_data/train_6mers.npy'\n","dataset = CS4220Dataset(input_file_path, label_df, samples_index=samples_index)\n","data = dataset.X\n","labels = dataset.Y\n","\n","best_kernel = 'rbf'\n","best_C = 1\n","\n","# scaler = StandardScaler()\n","# X_train_scaled = scaler.fit_transform(data)\n","\n","model = OneVsRestClassifier(SVC(kernel=best_kernel, C=best_C, probability=True))\n","model.fit(data, labels)"]},{"cell_type":"code","execution_count":9,"metadata":{},"outputs":[],"source":["from joblib import dump, load\n","# dump(model, 'models/svc_noscaler_c1.joblib')\n","# model = load('models/svc_noscaler.joblib')"]},{"cell_type":"code","execution_count":12,"metadata":{},"outputs":[],"source":["def jaccard_index_per_patient(patient_id, preds):\n","    df_true = pd.read_csv('test_data/patient{}_labels.csv'.format(patient_id))\n","    tp, fp, tp_fn = 0, 0, df_true['labels'].shape[0]\n","    print('my predition(s) for patient {}:'.format(patient_id))\n","    print(preds)\n","    print('true pathogen')\n","    print(df_true['labels'].values)\n","    # if don't predict any pathogen, it means there is only decoy in the test dataset (your prediction)\n","    if len(preds) == 0:\n","        preds = ['decoy']\n","    for item in np.unique(preds):\n","        if item in df_true['labels'].values:\n","            tp += 1\n","        else:\n","            fp += 1\n","    #you have to predict all labels correctly, but you are penalized for any false positive\n","    return tp / (tp_fn + fp)"]},{"cell_type":"code","execution_count":7,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["predicting for patient 0\n"]},{"ename":"NameError","evalue":"name 'model' is not defined","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m      9\u001b[0m     df_test \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mload(read_file)\n\u001b[1;32m     11\u001b[0m \u001b[38;5;66;03m# regr.predict relies on argmax, thus predict to every single read and you will end up with many false positives\u001b[39;00m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;66;03m# y_pred = model.predict(df_test)\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# we can use regr.predict_proba to find a good threshold and predict only for case where the model is confident.\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# here I apply 0.95 as the cutoff for my predictions, let's see how well my model will behave...\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m y_predprob \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241m.\u001b[39mpredict_proba(df_test)\n\u001b[1;32m     18\u001b[0m \u001b[38;5;66;03m# we get only predictions larger than the threshold and if there is more than one, we take the argmax again\u001b[39;00m\n\u001b[1;32m     19\u001b[0m final_predictions \u001b[38;5;241m=\u001b[39m le\u001b[38;5;241m.\u001b[39minverse_transform(\n\u001b[1;32m     20\u001b[0m                         np\u001b[38;5;241m.\u001b[39munique([np\u001b[38;5;241m.\u001b[39margmax(item) \u001b[38;5;28;01mfor\u001b[39;00m item \u001b[38;5;129;01min\u001b[39;00m y_predprob \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(np\u001b[38;5;241m.\u001b[39mwhere(item \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m threshold)[\u001b[38;5;241m0\u001b[39m]) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     21\u001b[0m                     ))\n","\u001b[0;31mNameError\u001b[0m: name 'model' is not defined"]}],"source":["#prediction for all patients\n","\n","threshold = 0.99\n","\n","all_jaccard_index = []\n","for patient_id in range(10):\n","    print('predicting for patient {}'.format(patient_id))\n","\n","    with open('test_data/patient{}_6mers.npy'.format(patient_id), 'rb') as read_file:\n","        df_test = np.load(read_file)\n","\n","    # regr.predict relies on argmax, thus predict to every single read and you will end up with many false positives\n","    # y_pred = model.predict(df_test)\n","\n","    # we can use regr.predict_proba to find a good threshold and predict only for case where the model is confident.\n","    # here I apply 0.95 as the cutoff for my predictions, let's see how well my model will behave...\n","    y_predprob = model.predict_proba(df_test)\n","\n","    # we get only predictions larger than the threshold and if there is more than one, we take the argmax again\n","    final_predictions = le.inverse_transform(\n","                            np.unique([np.argmax(item) for item in y_predprob if len(np.where(item >= threshold)[0]) >= 1]\n","                        ))\n","\n","    # my pathogens dectected, decoy will be ignored\n","    final_predictions = [item for item in final_predictions if item !='decoy']\n","\n","    ji = jaccard_index_per_patient(patient_id, final_predictions)\n","    print('Jaccard index: {}'.format(ji))\n","    all_jaccard_index.append(ji)"]},{"cell_type":"code","execution_count":16,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['patient 0: 0.125', 'patient 1: 0.09090909090909091', 'patient 2: 0.5', 'patient 3: 0.3333333333333333', 'patient 4: 0.08333333333333333', 'patient 5: 0.125', 'patient 6: 0.07692307692307693', 'patient 7: 0.2', 'patient 8: 0.2', 'patient 9: 0.1'] avg: 0.18344988344988344\n"]}],"source":["print(['patient {}: {}'.format(c,item) for c, item in enumerate(all_jaccard_index)], 'avg: {}'.format(np.mean(all_jaccard_index)))"]},{"cell_type":"code","execution_count":13,"metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["/Users/melloo21/opt/anaconda3/envs/cs4220_pathogen_detection/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator SVC from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/Users/melloo21/opt/anaconda3/envs/cs4220_pathogen_detection/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator LabelBinarizer from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n","/Users/melloo21/opt/anaconda3/envs/cs4220_pathogen_detection/lib/python3.8/site-packages/sklearn/base.py:348: InconsistentVersionWarning: Trying to unpickle estimator OneVsRestClassifier from version 1.2.2 when using version 1.3.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\n","https://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["predicting for patient 0\n","my predition(s) for patient 0:\n","['staphylococcus_aureus']\n","true pathogen\n","['staphylococcus_aureus']\n","Jaccard index: 1.0\n","predicting for patient 1\n","my predition(s) for patient 1:\n","['neisseria_gonorrhoeae', 'salmonella_enterica_typhimurium']\n","true pathogen\n","['staphylococcus_pyogenes']\n","Jaccard index: 0.0\n","predicting for patient 2\n","my predition(s) for patient 2:\n","['burkholderia_pseudomallei', 'corynebacterium_ulcerans']\n","true pathogen\n","['burkholderia_pseudomallei' 'corynebacterium_ulcerans']\n","Jaccard index: 1.0\n","predicting for patient 3\n","my predition(s) for patient 3:\n","['pseudomonas_aeruginosa']\n","true pathogen\n","['pseudomonas_aeruginosa']\n","Jaccard index: 1.0\n","predicting for patient 4\n","my predition(s) for patient 4:\n","['corynebacterium_diphtheriae']\n","true pathogen\n","['corynebacterium_diphtheriae']\n","Jaccard index: 1.0\n","predicting for patient 5\n","my predition(s) for patient 5:\n","['streptococcus_pneumoniae']\n","true pathogen\n","['streptococcus_pneumoniae']\n","Jaccard index: 1.0\n","predicting for patient 6\n","my predition(s) for patient 6:\n","['mycobacterium_ulcerans']\n","true pathogen\n","['mycobacterium_ulcerans']\n","Jaccard index: 1.0\n","predicting for patient 7\n","my predition(s) for patient 7:\n","['mycobacterium_tuberculosis', 'neisseria_gonorrhoeae']\n","true pathogen\n","['mycobacterium_tuberculosis' 'streptococcus_pneumoniae']\n","Jaccard index: 0.3333333333333333\n","predicting for patient 8\n","my predition(s) for patient 8:\n","['streptococcus_pneumoniae']\n","true pathogen\n","['streptococcus_pneumoniae']\n","Jaccard index: 1.0\n","predicting for patient 9\n","my predition(s) for patient 9:\n","['burkholderia_pseudomallei', 'neisseria_gonorrhoeae']\n","true pathogen\n","['burkholderia_pseudomallei']\n","Jaccard index: 0.5\n"]}],"source":["#prediction for all patients\n","model = load('models/svc_noscaler_c1.joblib')\n","threshold = 0.99\n","\n","all_jaccard_index = []\n","for patient_id in range(10):\n","    print('predicting for patient {}'.format(patient_id))\n","\n","    with open('test_data/patient{}_6mers.npy'.format(patient_id), 'rb') as read_file:\n","        df_test = np.load(read_file)\n","\n","    # regr.predict relies on argmax, thus predict to every single read and you will end up with many false positives\n","    # y_pred = model.predict(df_test)\n","\n","    # we can use regr.predict_proba to find a good threshold and predict only for case where the model is confident.\n","    # here I apply 0.95 as the cutoff for my predictions, let's see how well my model will behave...\n","    y_predprob = model.predict_proba(df_test)\n","\n","    # we get only predictions larger than the threshold and if there is more than one, we take the argmax again\n","    final_predictions = le.inverse_transform(\n","                            np.unique([np.argmax(item) for item in y_predprob if len(np.where(item >= threshold)[0]) >= 1]\n","                        ))\n","\n","    # my pathogens dectected, decoy will be ignored\n","    final_predictions = [item for item in final_predictions if item !='decoy']\n","\n","    ji = jaccard_index_per_patient(patient_id, final_predictions)\n","    print('Jaccard index: {}'.format(ji))\n","    all_jaccard_index.append(ji)"]},{"cell_type":"code","execution_count":14,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["['patient 0: 1.0', 'patient 1: 0.0', 'patient 2: 1.0', 'patient 3: 1.0', 'patient 4: 1.0', 'patient 5: 1.0', 'patient 6: 1.0', 'patient 7: 0.3333333333333333', 'patient 8: 1.0', 'patient 9: 0.5'] avg: 0.7833333333333333\n"]}],"source":["print(['patient {}: {}'.format(c,item) for c, item in enumerate(all_jaccard_index)], 'avg: {}'.format(np.mean(all_jaccard_index)))"]},{"cell_type":"code","execution_count":15,"metadata":{},"outputs":[{"data":{"text/plain":["['burkholderia_pseudomallei', 'neisseria_gonorrhoeae']"]},"execution_count":15,"metadata":{},"output_type":"execute_result"}],"source":["final_predictions"]},{"cell_type":"markdown","metadata":{"id":"SlQWHsVU57JR"},"source":["We can now save our trained model for later usage. This is an example of how you can send your model to our final evaluation."]},{"cell_type":"markdown","metadata":{"id":"LwWuRXTW57JS"},"source":["Then the model can be loaded using\n","\n","```python\n","# load trained model\n","regr = load('models/baseline.joblib')\n","```\n","\n","## Evaluation of Model\n","\n","Now that you have your trained model, you can use it on each of the patient's read dataset and try to find the pathogens that appear in each patient.\n","\n","Your model is evaluated based on [**Jaccard index**](https://en.wikipedia.org/wiki/Jaccard_index#Jaccard_index_in_binary_classification_confusion_matrices). For patient $i$, let $P$ be the set of pathogen species your model predicted (or $\\{\\text{decoy}\\}$ if there is no pathogens predicted), and $T$ the set of pathogen species the patient actually have (or $\\{\\text{decoy}\\}$ if there is no pathogens in the reads), the score for your model is\n","\n","$$\\text{Jaccard index}=\\frac{|P\\cap T|}{|P \\cup T|}$$"]},{"cell_type":"markdown","metadata":{"id":"Be0a5nLF57JU"},"source":["Going back to our model: since we are using logistic regression, our model will classify each read to the class that has the highest probability of having the read. If we report all the species that a read has been classified to, then we may end up with a lot of false positives (why?).\n","\n","One potential way to counter this is to define a threshold. Here I used 0.95: I only report a species if I am 95% confident that one read comes from that species. Let's see how well my model will behave..."]},{"cell_type":"markdown","metadata":{"id":"o5tEWYmN57JV"},"source":["So the overall score for my model is 0.45. Not a bad start, but still much room for improvement. You don't necessarily need to work on this baseline; this was just released as an example. Have fun!!!"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"cs4220_pathogen_detection","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.19"}},"nbformat":4,"nbformat_minor":0}
